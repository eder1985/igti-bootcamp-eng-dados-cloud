{
	"jobConfig": {
		"name": "glue-job-microdados-enem-2020",
		"description": "",
		"role": "arn:aws:iam::086556745643:role/service-role/AWSGlueServiceRole-teste-igti-glue-crawler",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue-job-microdados-enem-2020.py",
		"scriptLocation": "s3://datalake-eder-enem/scripts/",
		"language": "python-3",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2023-07-24T20:09:45.928Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://datalake-eder-enem/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"spark": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://datalake-eder-enem/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null
	},
	"hasBeenSaved": false,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\n\r\n## @params: [JOB_NAME]\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\n\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Ler os dados do arquivo fonte\r\nraw_data = spark.read.format('csv').option('header', True).option('inferSchema', True).option('delimiter', ';').load('s3://datalake-eder-enem/raw-data/')\r\n\r\n# Escrita dos dados no formato parquet\r\nraw_data.write.mode('overwrite').format('parquet').save('s3://datalake-eder-enem/consumer-zone/enem2020/')\r\n\r\n#job.commit()"
}